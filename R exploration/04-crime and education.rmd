---
title: 'IMT 573: Problem Set 4 - Data Analysis'
author: "Xinyi Yang"
date: 'Due: Tuesday, November 3, 2020'
output:
  pdf_document: default
  html_document: default
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: <!-- BE SURE TO LIST ALL COLLABORATORS HERE! -->

##### Instructions:

Before beginning this assignment, please ensure you have access to R and RStudio. 

1. Download the `problemset4.Rmd` file from Canvas. Open `problemset4.Rmd` in RStudio and supply your solutions to the assignment by editing `problemset4.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

4. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment. In particular, note that Stack Overflow is licenses as Creative Commons (CC-BY-SA). This means you have to attribute any code you refer from SO.

5. Partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. But please **DO NOT** submit pages and pages of hard-to-read code and attempts that is impossible to grade. That is, avoid redundancy. Remember that one of the key goals of a data scientist is to produce coherent reports that others can easily follow.  Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run without errors you can do so with the `eval=FALSE` option as follows:

```{r example chunk with a bug, eval=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

6. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `ps4_ourLastName_YourFirstName.pdf`, and submit the PDF file on Canvas.

7.  Collaboration is often fun and useful, but each student must turn in an individual write-up in their own words as well as code/work that is their own.  Regardless of whether you work with others, what you turn in must be your own work; this includes code and interpretation of results. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

##### Setup

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE, warning=FALSE}
# Load standard libraries
library(tidyverse)
library(gridExtra)
library(reshape2)
```

#### Problem 1: 50 States in the USA

In this problem we will use the `state` dataset, available as part of the R statistical computing platform. This data is related to the 50 states of the United States of America. Load the data and use it to answer the following questions. 

```{r load dataset, message=FALSE}
data(state)
```

  
##### (a) Describe the data and each variable it contains. Tidy the data, preparing it for a data analysis.  

...
  
\textcolor{purple}{RESPONSE:}
**data description**  
1. state.abb: character vector of 2-letter abbreviations for the state names.  

2. state.area: numeric vector of state areas (in square miles).  

3. state.center: list with components named x and y giving the approximate geographic center of each state in negative longitude and latitude. Alaska and Hawaii are placed just off the West Coast.  

4. state.division: factor giving state divisions (New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East North Central, West North Central, Mountain, and Pacific).

5. state.name: character vector giving the full state names.

6. state.region: factor giving the region (Northeast, South, North Central, West) that each state belongs to.

7. state.x77: matrix with 50 rows and 8 columns giving the following statistics in the respective columns.
    + Population: population estimate as of July 1, 1975
    + Income: per capita income (1974)
    + Illiteracy: illiteracy (1970, percent of population)
    + Life Exp: life expectancy in years (1969–71)
    + Murder: murder and non-negligent manslaughter rate per 100,000 population (1976)
    + HS Grad: percent high-school graduates (1970)
    + Frost: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city
    + Area: land area in square miles  
  \
  
  
```{r}
state <- data.frame(state.x77)

names(state) <- c("Population", "Income", "Illiteracy", "LifeExp", "Murder", "HSGrad", "Frost", "Area")
state$abb <-state.abb
#area not the same
  
state$name <- state.name
state$region <- state.region 
state$division <- state.division
```
  
...
  
  
##### (b) Suppose you want to explore the relationship between a state's `Murder` rate and other characteristics of the state, for example population, illiteracy rate, and more. Begin by examining the bivariate relationships present in the data. What does your analysis suggest might be important varibles to consider in building a model to explain variation in murder rates?  
  
...
```{r , warning=FALSE}

# draw scatter point plots to observe the bivariate relationship
# print the correlation between the two variates on the plot

# trash
# draw <- function(x){
#   ggplot(mapping = aes(x,Murder))+
#   geom_jitter()+
#   geom_smooth(method = 'glm')+
#   annotate("text", y=max(Murder),x=max(x),label = round(cor(x, Murder),3))
# }
# draw(Population)
# draw(Income)
# draw(Frost)
# draw(LifeExp)

df <- state %>% select(c("Population", "Income", "Illiteracy", "LifeExp", "HSGrad", "Frost","Murder"))

# create a table to store the correlation value
corr <- tibble(
  varialbe = names(df),
  correlation = rnorm(length(names(df)))
)

# use a loop to output the correlation
for (i in seq_along(df)){
  corr[i,2] <- cor(df[i],df$Murder)
}


# to plot these plots in grid area
# reference: https://stackoverflow.com/questions/21653269/use-columns-as-facets-in-graph

df.mlt <- melt(df, id.vars=c("Murder"))

ggplot(df.mlt, aes(x=Murder, y=value)) + 
  geom_jitter() +
  geom_smooth(method = 'glm')+
  facet_wrap(~ variable, scales="free_y") +
  theme(axis.text.x=element_text(angle=90))


corr
```



  
  
\textcolor{purple}{Response:}
I found two variables that might be important to consider in building a model to explain variation in murder rates, and they are `LifeExp`(life expectancy in years) and `Illiteracy`. The respective correlation shown on the plots is closer to 1 or -1.
  

##### (c) Develop a new research question of your own that you can address using the `state` dataset. Clearly state the question you are going to address. Provide at least one visualization to support your exploration of this question. Discuss what you find.
  
  
\textcolor{purple}{QUESTION:}  

What does the income distribution looks like in each region across the states? Do different cities in the same region have obivous gaps in their income?  

  
```{r}
# income distribution by region 
ggplot(state,aes(region, Income)) +
  geom_boxplot()

# how spread apart of the income of cities in each region
state %>% 
  group_by(region) %>% 
  summarise(var_income = var(Income), std_income = sd(Income))
```


```{r}
# just curious about that single point in West
state %>% 
  filter(region == 'West') %>% 
  slice_max(Income)
```
  
\textcolor{purple}{RESPONSE:}  
I wish I could do this analysis on newer data. Within the four regions, the west region has the widest spread of income. And the gap between different cities in the region of north central is the smallest.
  
\textcolor{purple}{TO SATISFY MY CURIOSITY}
  
**Does people's income have a relationship with their Illiteracy?**

   
```{r}
attach(state)
ggplot(mapping = aes(Illiteracy,Income))+
  geom_jitter()+
  geom_smooth(method = 'glm')+
  annotate("text", y=max(Murder),x=max(Illiteracy),label = round(cor(Illiteracy, Murder),3))
```
  
  
\textcolor{purple}{RESPONSE:}
Although the correlation between illiteracy and income is not very close to 1, there's a vague trend that cities with higher illiteracy earn less income.
  
**Just want to make a visualization of the murder rate of all the cities in a descending order and colored by regions.**
  

```{r }  
df <- state %>% arrange(desc(Murder))
ggplot(df, aes(abb,Murder))+
  geom_bar(aes(fill=region) ,stat = 'identity', width = 0.5)+
  xlim(df$abb)+
  labs(title="Murder rate of all major cities colored by region")+
  xlab("Cities")+
  ylab("Murder rate (per population)")+
  theme(axis.text.x = element_text(angle = 90),
        legend.position = 'bottom')
```


#### Problem 2: Asking Data Science Questions: Crime and Educational Attainment

In Problem Set 3, you joined data about crimes and educational attainment. Here you will use this new combined dataset to examine questions around crimes in Seattle and the educational attainment of people living in the areas in which the crime occurred. 


#### (a) Develop a Data Science Question

Develop your own question to address in this analysis. Your question should be specific and measurable, and it should be able to be addressed through a basic analysis of the crime dataset you compiled in Problem Set 3. 

  
\textcolor{purple}{QUESTION:}
Do beats with more people who have been to college have less murders occur? 

#### (b) Describe and Summarize

Briefly summarize the dataset, describing what data exists and its basic properties. Comment on any issues that need to be resolved before you can proceed with your analysis. 


\textcolor{purple}{The dataset is about the reports of crimes in the Seattle area, and the corresponding education information in the area. It has the time, category, place of the crime, and how many people are to what level educated.}  
  
  
```{r}
# load the crimes dataset
crime <- read.csv("crime_beats_census.csv")
# remove the original index column
crime <- crime %>%  select(-c('X'))

# the columns
names(crime)

# the data type of each column
str(crime)

# check the contents of Sector
unique(crime$Sector)

# clean the data
crime <- crime[!(is.na(crime$Sector) | crime$Sector=="" |crime$Sector=='6804'), ]

```

  

#### (c) Data Analysis

Use the dataset to provide empirical evidence that addressed your question from part (a). Discuss your results. Provide at least one visualization to support your narrative. 
  
  
  
```{r}
# create a vector of all the high-education columns
colnms=c('bachelors_degree','masters_degree',	'professional_school_degree','doctorate_degree')

crime_df <- crime %>% 
  # sum the number of the high-education people by rows, remove NAs
  mutate(HighEdu = rowSums(crime[,colnms],na.rm = TRUE)) %>%
  # select the columns I might use to reduce runtime
  select(Report.Number,Sector,Beat, total,HighEdu) %>% 
  # calculate the proportion of the high-education people among the whole population 
  mutate(HighEduperPpl=HighEdu/total) 

# create another dataframe grouped by Beat
crime_df2 <- crime_df %>% 
  group_by(Beat) %>% 
    # calculate the mean proportion of high-education people in each Beat
  summarise(meanEdu=mean(HighEduperPpl))

# create a small dataframe with Beat and number of murders occurred
df <- crime %>% count(Beat) %>% arrange(desc(n)) %>% 
  # match the education variable
  left_join(crime_df2)

# create a Sector-Beat sheet 
StoB <- unique(crime_df %>% select(Sector,Beat))

# match the Sector
dff <- df %>% left_join(StoB)

# set the xais and annotation before-hand
xaxis <- crime %>% count(Beat) %>% arrange(desc(n))
corr <- paste("correlation: " ,round(cor(dff$n,dff$meanEdu),3))

ggplot(dff,aes(x=Beat)) +
  geom_bar(aes(y=n,fill=Sector),stat = 'identity',width = 0.7) +
  geom_point(aes(y=meanEdu*10000),color='black',size=1) + # multiple by 10000 to make them visible on the same plot
  theme(axis.text.x = element_text(angle = 90),
        legend.position = 'right') +
  scale_y_continuous(
    name = "murder count",
    sec.axis = sec_axis(~.*0.0001,name = "mean of high-Education proportion") 
    # adjust the axis to show the real value of the education variable
  ) +
  scale_x_discrete(limits=xaxis$Beat) +
  annotate("text",y=max(dff$n),x=40,label = corr)

```
  
    
\textcolor{purple}{RESPONSE:}
Beats with more people who have been to college don't have less murders occur. And the correlation between the proportion of people whose education level are higher than bachelor's degree and the number of murders is low. We can also observe the results from the visualization above. Although the bar chart is ordered by the number of murders, the points are splattered on the plot messily. Maybe people just don't do such malicious things in their neighborhood. I should look more into places where the illiteracy is higher. 

#### (d) Reflect and Question

Comment the questions (and answers) in this analysis.  Were you able to answer all of these questions?  Are all questions well defined?  Is the data good enough to answer all these?

\textcolor{purple}{RESPONSE:}
When answering the question of bivariate relationships, I felt a little bit confused because I don't know how to observe the relationship. I don't think we did this in class or in lab. The crime and education dataset is not organized enough to answer my question. I need to preprocess the dataset first, adding several columns together. 
